---
title: "hanqin_group_lasso"
author: "Hanqin Du"
date: "2020/1/11"
output: html_document
---



```{r set work space for r, eval=FALSE, echo=FALSE}
par(mfrow=c(1,1))
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
```



# prepare data
***

### Load library
Load library for data frame organization, training linear model and plotting
```{r load library, results='hide', message=FALSE, echo=FALSE}
library(data.table)
library(tidyverse)
library(lmodel2)
library(splitstackshape)
library(gridExtra)
library(glmnet)
```

### Load function
```{r results='hide', message=FALSE, echo=FALSE}
strReverse <- function(x)
        sapply(lapply(strsplit(x, NULL), rev), paste, collapse="")
  
plot_labels = function(labels, ylim){
  plot(1:length(labels)[1], type = 'n', xlab="",ylab="coefficient",xaxt="n",xlim = c(0.5,length(labels)[1]+0.5), ylim = ylim)+
    axis(side=1,at=1:length(labels)[1],labels=labels) + abline(h=0, col="grey")
}

plot_points <- function(data, color){
  points(x = 1:length(data)[1],y = data,pch = 16,col = color)
}

plot_line <- function(data, color){
  lines(x = 1:length(data)[1], y = data, pch = 16, col = color)
}

range_coe <- function(beta_list){
  n = length(beta_list)
  minimum = 0
  maximum = 0
  for (i in (1:n)) {
    minimum = min(minimum, min(beta_list[[i]]))
    maximum = max(maximum, max(beta_list[[i]]))
  }
  return(c(minimum,maximum))
}

mask_characters <- function(c1,c2){
  n = length(c1)
  m = length(c2)
  output = c()
  for (i in 1:n) {
    for (j in 1:m) {
      if(c1[i] == c2[j]){
        output = append(output,i)
      }
    }
  }
  return(output)
}

```


### Load Data
Load 69 published motifs (from Abhi's report) and 3'UTR sequences of 4388 different genes. Then, Load Gasch's gene-expression-level data which describes the relative gene expression level under 173 different environment conditions among 6152 genes.

**a problem with Gasch's data: there are three duplicated column called 'hs_00min_hs-2', need to check them in the article**
```{r Load Data, echo=FALSE, warning=FALSE, message=FALSE}

#ref datasets for UTRs 
UTR_raw <- read_rds("../data/Sun_mutation_UTRs.rds")
  #Get sequences from UTR_raw in a separate vector
  UTR_3 <- UTR_raw$UTR3_seq

#Load Manually created motifs list into a vector
motifs_raw <- scan("../data/list_motifs.txt", character())
motifs_cheng = c("TGTAAATA", "TGCAT", "TTTTTTA", "ATATTC")


# load Gasch's data
expressionLevel_Gasch <- read_tsv("../data/Gasch2000_complete_dataset_rename.txt", 
                       locale = locale(decimal = ","))

# convert all the expression data to number
for (i in names(expressionLevel_Gasch)[3:176]) {
  expressionLevel_Gasch[[i]] <- as.double(as.character(expressionLevel_Gasch[[i]]))
}

```

### Construct Motif Frequencies Matrix from 3'UTR Ref sequences
By converting all the motifs to regular expression, we are able to construct motifs frequency matrix which describes the frequency of 69 motifs among 4388 different genes.
```{r construct motif frequency, echo=FALSE}

#Dictionary for non-specific codes and converting U -> T
motifs <- motifs_raw %>% str_replace_all(c("U" = "T", "W" = "(A|T)", "S" = "(C|G)", "M" = "(A|C)", "K" = "(G|T)", "R" = "(A|G)", "Y" = "(C|T)", "B" = "(C|G|T)", "D" = "(A|G|T)", "H" = "(A|C|T)", "V" = "(A|C|G)", "N" = "(A|C|G|T)"))

#Initate ref tibble and store gene names
ref_motifs <- tibble(geneName = UTR_raw$genename)


#Search and add frequency of each c(motif) as a column in ref dataset
for (i in 1:length(motifs)){
  ref_motifs <- mutate(.data = ref_motifs,!!motifs_raw[i] := str_count(UTR_3, motifs[i]))
}

#Search and add frequency of each c(motif) as a column in ref dataset
for (i in 1:length(motifs)){
  ref_motifs <- mutate(.data = ref_motifs,!!motifs_raw[i] := str_count(UTR_3, motifs[i]))
}

names_motifs_all <- names(ref_motifs)[2:length(ref_motifs)]
```




### Merge motif frequency matrix with Gasch's data
Finally, we inner join the motif frequency matrix with Gasch's data and get the data frame we are going to explore. Each row represented data from one of the 4284 genes (since we have carried out inner join, only the genes shown in both tables are kept). There are 245 columns in total. 3 of them describe the basic information (short and formal name) of the gene represented by the row. 173 of them show how the gene expression level change under the certain environment condition and the other 69 of them respect the frequency of the 69 published motifs on the 3’UTR.
```{r merge motifs frequency with gasch data, echo=FALSE}

# merge motifs
fullTable_Gasch <- merge(expressionLevel_Gasch,ref_motifs,by = "geneName")

# convert type of motifs frequency to factor for violin plotting
fullTable_Gasch_double = fullTable_Gasch
for(i in names_motifs_all){
    fullTable_Gasch_double[[i]] <- as.double(as.character(fullTable_Gasch_double[[i]]))
}

```


### Remove motifs with frequency 0
```{r echo=FALSE}
motifs_count_sum <- colSums(fullTable_Gasch[names_motifs_all],na.rm=TRUE)
remove_list = NULL

for (i in names(motifs_count_sum)){
  if (motifs_count_sum[[i]] == 0){
    remove_list <- c(remove_list,i)
  }
}

names_motifs_valid <- names_motifs_all[!names_motifs_all %in% remove_list]
```




# multi-task learning
***


### absic idea about glmnet group lasso

reference:

- https://www.rdocumentation.org/packages/glmnet/versions/3.0-2/topics/glmnet
- https://cran.r-project.org/web/packages/glmnet/glmnet.pdf
- https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html
- Jerome Friedman et al, A note on the group lasso and a sparse group lasso
- https://en.wikipedia.org/wiki/Lasso_(statistics)#Group_lasso
- https://www.zhihu.com/question/38121173
- https://zhuanlan.zhihu.com/p/46999826
- https://cosx.org/2016/10/data-mining-1-lasso/
- How to interpret cv.glmnet() plot?: https://stats.stackexchange.com/questions/253963/how-to-interpret-cv-glmnet-plot


defination of p-norm
$$
||x||_p := (\sum ^n_{i=1}|x_i|^p)^{1\over p}
$$


> Glmnet is a package that fits a generalized linear model via penalized maximum likelihood. The regularization path is computed for the lasso or elasticnet penalty at a grid of values for the regularization parameter lambda.

$$
\min_{\beta_0,\beta} \frac{1}{N} \sum_{i=1}^{N} w_i l(y_i,\beta_0+\beta^T x_i) + \lambda\left[(1-\alpha)||\beta||_2^2/2 + \alpha ||\beta||_1\right],
$$
Where l(y,η) is the negative log-likelihood contribution for observation i.

accroding to the introduction of glmnet, the penalty on the coefficient vector for variable j is
$$
(1-\alpha)/2||\beta_j||_2^2+\alpha||\beta_j||_1.
$$

Setting `family="mgaussian"` allows a multi-response gaussian model to be fit, using a "group -lasso" penalty on the coefficients for each variable.
Setting `type.multinomial="grouped"` allows the same penalty as `family="mgaussian"`, which is:
$$
(1-\alpha)/2||\beta_j||_2^2+\alpha||\beta_j||_2.
$$
when `alpha = 1` this is a group-lasso penalty, and otherwise it mixes with quadratic just like elasticnet. In group lasso, we summing all the penalty


### compare L1 and L2

- L1 could have more than one optimal solution while L2 only has one
- L1 is more robust. no sensitive to extreme value
- L1 give more 0 to not important features (better for feature selection)
- L2 is easier to calculate




### test glmnet group lasso with a group of heat-shock conditions
alpha = 1 for group lasso


```{r}
names_temperature_condition = vector(mode="character", length=5)
names_temperature_condition[1] = "hs_05min_hs-1"
names_temperature_condition[2] = "hs_10min_hs-1"
names_temperature_condition[3] = "hs_15min_hs-1"
names_temperature_condition[4] = "hs_30min_hs-1"
names_temperature_condition[5] = "hs_40min_hs-1"
names_temperature_condition[6] = "hs_60min_hs-1"
names_temperature_condition[7] = "hs_80min_hs-1"

x_hs = as.matrix(fullTable_Gasch_double[names_motifs_valid])
y_hs = as.matrix(fullTable_Gasch_double[names_temperature_condition])

```


how to deal with the N/A number in independent variable(x) and dependent variable(y)?
> it is easy to ignore N/A in x by setting them as 0. However, for dependent variable, we would prefer to ingore the loss cause by that data. Setting it as 0 won't have the same effect. Temporary, we set it as 0 and try to improve it latter.





```{r}
x_hs[is.na(x_hs)] = 0.0
y_hs[is.na(y_hs)] = 0.0

cv_models_hs = cv.glmnet(x_hs, y_hs, family = "mgaussian")
```


### pick lambda with 10-fold cross validation

```{r echo=FALSE}
plot(cv_models_hs)
```

there are two different lambdas `lambda.min` and `lambda.1se`, which one to used?
`lambda.min` is the value of lambda that gives minimum cvm.
`lambda.1se` is the largest value of lambda such that error is within 1 standard error of the minimum.

meaning of the error bar?
> Confidence intervals represent error estimates for the loss metric (red dots). They're computed using CV. The vertical lines show the locations of λmin and λ1se. The numbers across the top are the number of nonzero coefficient estimates.


how the mse is calculated?
> The error is accumulated, and the average error and standard deviation over the folds is computed


why alpha = 1?
> When alpha=1 this is a group-lasso penalty, and otherwise it mixes with quadratic just like elasticnet.



### training models with the lambda value that gives minimum loss
```{r}
models_hs = glmnet(x_hs, y_hs, family = "mgaussian", lambda = cv_models_hs$lambda.min)
```


### compare their coefficient
```{r echo=FALSE, fig.height=12, fig.width=12}
{
  plot_labels(names_motifs_valid, range_coe(models_hs$beta))
  plot_points(models_hs$beta$`hs_05min_hs-1`,'black')
  plot_points(models_hs$beta$`hs_10min_hs-1`,'gray15')
  plot_points(models_hs$beta$`hs_15min_hs-1`,'gray30')
  plot_points(models_hs$beta$`hs_30min_hs-1`,'gray45')
  plot_points(models_hs$beta$`hs_40min_hs-1`,'gray60')
  plot_points(models_hs$beta$`hs_60min_hs-1`,'gray75')
  plot_points(models_hs$beta$`hs_80min_hs-1`,'gray90')
  legend('bottomleft', legend=c('5min','10min','15min', '30min','40min','60min','80min'), col=c('black', 'gray15', 'gray30', 'gray45', 'gray60','gray75','gray90'), lty = 1)
}

{
  plot_labels(names_motifs_valid, range_coe(models_hs$beta))
  plot_line(models_hs$beta$`hs_05min_hs-1`,'black')
  plot_line(models_hs$beta$`hs_10min_hs-1`,'gray15')
  plot_line(models_hs$beta$`hs_15min_hs-1`,'gray30')
  plot_line(models_hs$beta$`hs_30min_hs-1`,'gray45')
  plot_line(models_hs$beta$`hs_40min_hs-1`,'gray60')
  plot_line(models_hs$beta$`hs_60min_hs-1`,'gray75')
  plot_line(models_hs$beta$`hs_80min_hs-1`,'gray90')
  legend('bottomleft', legend=c('5min','10min','15min', '30min','40min','60min','80min'), col=c('black', 'gray15', 'gray30', 'gray45', 'gray60','gray75','gray90'), lty = 1)
}

```

### compare the coefficients with the linear model without group lasso
```{r}
x_hs = as.matrix(fullTable_Gasch_double[names_motifs_valid])
y_hs = as.matrix(fullTable_Gasch_double$`hs_15min_hs-1`)
x_hs[is.na(x_hs)]<-0.0
y_hs[is.na(y_hs)]<-0.0

model_hs = glmnet(x_hs, y_hs, family = "gaussian", lambda = 0)
```

```{r echo=FALSE, fig.height=12, fig.width=12}
{
  plot_labels(names_motifs_valid, range_coe(list(models_hs$beta[[1]], model_hs$beta)))
  plot_line(model_hs$beta@x,'blue')
  plot_line(models_hs$beta$`hs_15min_hs-1`,'red')
  legend('bottomleft', legend=c('with lasso', 'without lasso'), col=c('red','blue'), lty = 1)
}


```


# multi-task learning on all 173 environmental stress
***

```{r}
names_condition = colnames(expressionLevel_Gasch, do.NULL = TRUE, prefix = "col")[4:176]

x = as.matrix(fullTable_Gasch_double[names_motifs_valid])
y = as.matrix(fullTable_Gasch_double[names_condition])

x[is.na(x)]<-0.0
y[is.na(y)]<-0.0

cv_models_all = cv.glmnet(x, y, family = "mgaussian")
```


### pick lambda with 10-fold cross validation
```{r}
plot(cv_models_all)
```

### training models with the lambda value that gives minimum loss
```{r}
models_all = glmnet(x, y, family = "mgaussian", lambda = cv_models_all$lambda.min)
```

### compare with the base line
```{r}
fullTable_Gasch_na_to_zero = fullTable_Gasch
fullTable_Gasch_na_to_zero[is.na(fullTable_Gasch_na_to_zero)] <- 0
sum(sapply(fullTable_Gasch_na_to_zero[4:176], var))
```


### compare their coefficient
```{r echo=FALSE, fig.height=12, fig.width=12}
{
  plot_labels(names_motifs_valid, range_coe(models_all$beta))
  plot_points(models_all$beta$`hs_05min_hs-1`,'black')
  plot_points(models_all$beta$`hs_10min_hs-1`,'gray15')
  plot_points(models_all$beta$`hs_15min_hs-1`,'gray30')
  plot_points(models_all$beta$`hs_30min_hs-1`,'gray45')
  plot_points(models_all$beta$`hs_40min_hs-1`,'gray60')
  plot_points(models_all$beta$`hs_60min_hs-1`,'gray75')
  plot_points(models_all$beta$`hs_80min_hs-1`,'gray90')
  legend('bottomleft', legend=c('5min','10min','15min', '30min','40min','60min','80min'), col=c('black', 'gray15', 'gray30', 'gray45', 'gray60','gray75','gray90'), lty = 1)
}

{
  plot_labels(names_motifs_valid, range_coe(models_all$beta))
  plot_line(models_all$beta$`hs_05min_hs-1`,'black')
  plot_line(models_all$beta$`hs_10min_hs-1`,'gray15')
  plot_line(models_all$beta$`hs_15min_hs-1`,'gray30')
  plot_line(models_all$beta$`hs_30min_hs-1`,'gray45')
  plot_line(models_all$beta$`hs_40min_hs-1`,'gray60')
  plot_line(models_all$beta$`hs_60min_hs-1`,'gray75')
  plot_line(models_all$beta$`hs_80min_hs-1`,'gray90')
  legend('bottomleft', legend=c('5min','10min','15min', '30min','40min','60min','80min'), col=c('black', 'gray15', 'gray30', 'gray45', 'gray60','gray75','gray90'), lty = 1)
}

```

### key coefficient
recall that the four motifs we found with high correlation coefficient are `UGUAHMNUA`, `ATATTC`, `TGTATAWT`, and `TGTAAATA`. Only two of them are here.
```{r echo=FALSE, fig.height=12, fig.width=14}
names_motifs_key = c("UGUAHMNUA", "UKWCGRGGN", "CGGGTAAG","TGTATAWT","TTTTCTAGGDD","WAAAGGTAGTAAGT","TGAGGGCTA")
position_motifs_key = mask_characters(names_motifs_valid, names_motifs_key)


{
  plot_labels(names_motifs_key, range_coe(models_all$beta))
  plot_points(models_all$beta$`hs_05min_hs-1`[position_motifs_key],'black')
  plot_points(models_all$beta$`hs_10min_hs-1`[position_motifs_key],'gray15')
  plot_points(models_all$beta$`hs_15min_hs-1`[position_motifs_key],'gray30')
  plot_points(models_all$beta$`hs_30min_hs-1`[position_motifs_key],'gray45')
  plot_points(models_all$beta$`hs_40min_hs-1`[position_motifs_key],'gray60')
  plot_points(models_all$beta$`hs_60min_hs-1`[position_motifs_key],'gray75')
  plot_points(models_all$beta$`hs_80min_hs-1`[position_motifs_key],'gray90')
  legend('bottomleft', legend=c('5min','10min','15min', '30min','40min','60min','80min'), col=c('black', 'gray15', 'gray30', 'gray45', 'gray60','gray75','gray90'), lty = 1)
}

```



### Analysis the Bias
The bias of each model shows the change of expression level that cannot be explained by these motifs
here we calculate the percentage of it against the mean of change in expression level to estimate how much of the regulation effect cannot be expalined by the model. The estimation is base on many assumptions and could be very ambiguous. 

$$
bias\over mean( expression\ level)
$$


```{r} 
models_all$a0/colMeans(fullTable_Gasch_double[names_condition], na.rm = TRUE)
```


### compare coefficient under different type of environmental stress
```{r echo=FALSE, fig.height=12, fig.width=12}
{
  plot_labels(names_motifs_valid, range_coe(models_all$beta))
  plot_points(models_all$beta$`hs_15min_hs-1`,'red')
  plot_points(models_all$beta$`hs_37to25_15min`,'blue')
  legend('bottomleft', legend=c('heat shock','cool down'), col=c('red','blue'), lty = 1)
}

{
  plot_labels(names_motifs_valid, range_coe(models_all$beta))
  plot_line(models_all$beta$`hs_15min_hs-1`,'red')
  plot_line(models_all$beta$`hs_37to25_15min`,'blue')
  legend('bottomleft', legend=c('heat shock','cool down'), col=c('red','blue'), lty = 1)
}

```



### compare coefficient of heat shock with and without sorbital - 5min
check how the `additional effect` mension by Gasch shown in the model
```{r echo=FALSE, fig.height=12, fig.width=12}
{
  plot_labels(names_motifs_valid, range_coe(models_all$beta))
  plot_points(models_all$beta$`hs_29to33_05min`,'red')
  plot_points(models_all$beta$`29C(1M_sorbitol)~33C(1M_sorbitol)_05min`,'blue')
  plot_points(models_all$beta$`1M_sorbitol_05min`,'forestgreen')
  legend('bottomleft', legend=c('hs with sorbitol','hs without sorbitol','sorbitol without hs'), col=c('red','blue','forestgreen'), lty = 1)
}

{
  plot_labels(names_motifs_valid, range_coe(models_all$beta))
  plot_line(models_all$beta$`hs_29to33_05min`,'red')
  plot_line(models_all$beta$`29C(1M_sorbitol)~33C(1M_sorbitol)_05min`,'blue')
  plot_line(models_all$beta$`1M_sorbitol_05min`,'forestgreen')
  legend('bottomleft', legend=c('hs with sorbitol','hs without sorbitol','sorbitol without hs'), col=c('red','blue','forestgreen'), lty = 1)
}
```




### compare coefficient of heat shock with and without sorbital - 15min
```{r echo=FALSE, fig.height=12, fig.width=12}
{
  plot_labels(names_motifs_valid, range_coe(models_all$beta))
  plot_points(models_all$beta$`hs_29to33_15min`,'red')
  plot_points(models_all$beta$`29C(1M_sorbitol)~33C(1M_sorbitol)_15min`,'blue')
  plot_points(models_all$beta$`1M_sorbitol_15min`,'forestgreen')
  legend('bottomleft', legend=c('hs','sorbitol','hs + sorbitol'), col=c('red','forestgreen','blue'), lty = 1)
}


{
  plot_labels(names_motifs_valid, range_coe(models_all$beta))
  plot_line(models_all$beta$`hs_29to33_15min`,'red')
  plot_line(models_all$beta$`29C(1M_sorbitol)~33C(1M_sorbitol)_15min`,'blue')
  plot_line(models_all$beta$`1M_sorbitol_15min`,'forestgreen')
  legend('bottomleft', legend=c('hs','sorbitol','hs + sorbitol'), col=c('red','forestgreen','blue'), lty = 1)
}
```





### compare coefficient of heat shock with and without sorbital - 30min
```{r echo=FALSE, fig.height=12, fig.width=12}
{
  plot_labels(names_motifs_valid, range_coe(models_all$beta))
  plot_points(models_all$beta$`hs_29to33_30min`,'red')
  plot_points(models_all$beta$`29C(1M_sorbitol)~33C(1M_sorbitol)_30min`,'blue')
  plot_points(models_all$beta$`1M_sorbitol_30min`,'forestgreen')
  legend('bottomleft', legend=c('hs','sorbitol','hs + sorbitol'), col=c('red','forestgreen','blue'), lty = 1)
}


{
  plot_labels(names_motifs_valid, range_coe(models_all$beta))
  plot_line(models_all$beta$`hs_29to33_30min`,'red')
  plot_line(models_all$beta$`29C(1M_sorbitol)~33C(1M_sorbitol)_30min`,'blue')
  plot_line(models_all$beta$`1M_sorbitol_30min`,'forestgreen')
  legend('bottomleft', legend=c('hs','sorbitol','hs + sorbitol'), col=c('red','forestgreen','blue'), lty = 1)
}
```




### how to improve model?

- We should solve the problem we have in y's N/A value. There could be two way to deal with it. First, we fill up the N/A value by referencing other value. Second, we adjust the glmnet to ignore data with N/A y value.
- We may need to consider the relatively position of motifs
- We may need to consider the coperation between two RNA binding proteins or between RNA binding proteins and secondary structure
- May run cv.glmnet many times to reduce randomness
- adjust dependended value?
- more data?
- should we reflect the motifs and match the sequence?
- compare similar model to make the conclusion more reliable


### time table

- week 3: Apply basis function to explore if the effect of motifs combination. Analyse the models to quantify how the motifs affect regulation
- week 4: Expand the dataset.
- week 5: prepare for group meeting
- week 6-9: more attempt and improvement
- week 10-11: final report
- week 12-14: final presentations 



markdown report: what learning from existing motifs
explain what the model tell us about these motifs
fitting model - use the model as the central part, indicate how to improve and learning from it
feature selection: which motifs make effort

